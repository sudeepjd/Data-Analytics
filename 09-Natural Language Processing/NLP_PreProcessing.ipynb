{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_PreProcessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBAgFjm076AlWp4qALwAP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudeepjd/Data-Analytics/blob/master/09-Natural%20Language%20Processing/NLP_PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fICJ22rfgwr9",
        "colab_type": "text"
      },
      "source": [
        "# Data Pre-Processing for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH4KqvWYg9Oh",
        "colab_type": "text"
      },
      "source": [
        "## Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uuwJ74vgStg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "8fb19f1d-bbf7-4a35-a00b-31a599eab478"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt') #For word splitting\n",
        "nltk.download('averaged_perceptron_tagger') #For POS tagging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qopZnWc6hxW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent = \"The quick brown fox jumped over the lazy dog\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Yuj131xcFu",
        "colab_type": "text"
      },
      "source": [
        "## To Uppercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh67qL_2xS59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ec92daba-edb6-4535-c920-4fde89086441"
      },
      "source": [
        "sent_upper = sent.upper()\n",
        "print(sent_upper)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "THE QUICK BROWN FOX JUMPED OVER THE LAZY DOG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWtEZmSsxfIe",
        "colab_type": "text"
      },
      "source": [
        "## To Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1BlmwB-xiFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e70c68c9-ed8e-4842-9d7e-d1471d8ef3de"
      },
      "source": [
        "sent_lower = sent.lower()\n",
        "print(sent_lower)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the quick brown fox jumped over the lazy dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJKCT9obhHiy",
        "colab_type": "text"
      },
      "source": [
        "##  Tokenise to words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edjApNmShSY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e4435aa6-326d-40f5-a48b-ef14cf9a496c"
      },
      "source": [
        "words = nltk.word_tokenize(sent)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdop3jIIwQoN",
        "colab_type": "text"
      },
      "source": [
        "## Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRFjXSqMwTxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4f515fda-1784-4744-9e01-29ae3b3e18e7"
      },
      "source": [
        "words = nltk.word_tokenize(sent)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "print(pos_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9XxleKoytKP",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ay6x-8Hywmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "89290424-8811-4470-9fe1-28d41e01c607"
      },
      "source": [
        "grammar = \"NP: {<DT>?<JJ>*<NN>*}\"\n",
        "words = nltk.word_tokenize(sent)\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(pos_tags)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN fox/NN)\n",
            "  jumped/VBD\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXrsnjwa1GcZ",
        "colab_type": "text"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eyzD7Qx1clT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "43470eb3-b41b-4315-99f1-25ac5886ef3e"
      },
      "source": [
        "sent = \"Laziness is not going to be any form of governance for women\"\n",
        "words = nltk.word_tokenize(sent)\n",
        "\n",
        "porter = nltk.PorterStemmer()\n",
        "sent_porter_stem = [porter.stem(w) for w in words]\n",
        "print (\"-- Porter Stemmer\")\n",
        "print(sent_porter_stem)\n",
        "\n",
        "lancester = nltk.LancasterStemmer()\n",
        "sent_lanc_stem = [lancester.stem(w) for w in words]\n",
        "print (\"\\n-- Lancester Stemmer\")\n",
        "print(sent_lanc_stem)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Porter Stemmer\n",
            "['lazi', 'is', 'not', 'go', 'to', 'be', 'ani', 'form', 'of', 'govern', 'for', 'women']\n",
            "\n",
            "-- Lancester Stemmer\n",
            "['lazy', 'is', 'not', 'going', 'to', 'be', 'any', 'form', 'of', 'govern', 'for', 'wom']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqGQ3Q9E31sn",
        "colab_type": "text"
      },
      "source": [
        "## Remove Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3mSc1jM34Ei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "45acf0e8-cf2d-4692-985f-bcb3acd9876e"
      },
      "source": [
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print('-- Stopwords')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "print(stop_words)\n",
        "\n",
        "print(\"\\n-- Removed Stop Words\")\n",
        "filtered_words = [w for w in words if not w in stop_words]\n",
        "print(words)\n",
        "print(filtered_words) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Stopwords\n",
            "{\"should've\", 'had', 'more', \"you've\", 'while', \"you'll\", \"didn't\", 'me', 'all', 'mustn', 'other', 'o', 'up', 'yours', 'of', 'shouldn', 'each', 'down', 'once', 'into', 'aren', 'mightn', 'is', 'his', 'just', 'yourself', 'them', 'wouldn', 'couldn', \"mustn't\", 'wasn', 've', 'ours', 'didn', 'themselves', 'very', \"shouldn't\", 'such', 'my', 'don', 'll', 'about', 'shan', 'why', 'are', 'did', 'at', \"isn't\", 'haven', 'itself', 'was', 'herself', 'will', 'if', 'too', 'so', 'been', 'needn', 'being', 'd', 'which', 'what', 'on', 'when', 'through', 'ma', 'and', 'during', 'having', 'the', 'its', 'with', 'most', 'few', 'has', 'here', 't', 'by', 'hadn', 'between', 'over', 'or', \"that'll\", \"wasn't\", 'hers', 'for', 'whom', 'those', 's', 'before', 'not', 'our', 'from', 'your', \"couldn't\", \"doesn't\", \"weren't\", 'now', 'should', 'nor', \"don't\", 'ourselves', 'but', \"haven't\", 'who', 'does', 'against', 'out', 'how', 'only', 'he', \"aren't\", 'after', 'do', 'an', 'any', 'weren', 'doesn', 'i', 'theirs', \"you'd\", 'some', 'yourselves', 're', 'these', 'm', 'this', \"hadn't\", \"won't\", 'where', 'she', 'am', 'own', 'isn', 'until', 'ain', 'have', 'their', 'in', 'can', 'a', 'won', 'it', 'that', 'myself', \"needn't\", \"hasn't\", 'off', 'be', 'him', 'they', 'both', 'to', \"wouldn't\", 'then', \"you're\", 'as', 'you', 'than', 'no', 'hasn', \"it's\", \"mightn't\", 'were', 'below', \"she's\", 'her', 'under', 'same', 'doing', \"shan't\", 'because', 'y', 'above', 'himself', 'further', 'again', 'we', 'there'}\n",
            "\n",
            "-- Removed Stop Words\n",
            "['Laziness', 'is', 'not', 'going', 'to', 'be', 'any', 'form', 'of', 'governance', 'for', 'women']\n",
            "['Laziness', 'going', 'form', 'governance', 'women']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWEqsK_dN6DK",
        "colab_type": "text"
      },
      "source": [
        "## Bag of Words - Words to Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbp-vwq5N_ib",
        "colab_type": "text"
      },
      "source": [
        "### Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdyXno3eN-wY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "cdbc65ca-4cfb-4711-fdae-1ed54cb5b03d"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "\n",
        "# create the transform\n",
        "cv = CountVectorizer()\n",
        "vector = cv.fit_transform(text).toarray()\n",
        "\n",
        "# summarize\n",
        "print(\"-- Vocab\")\n",
        "print(cv.vocabulary_)\n",
        "print(\"\\n-- Vector\")\n",
        "print(vector)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Vocab\n",
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "\n",
            "-- Vector\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCFy9akZOdsP",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocAugAZ5Ohqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "b70e6da9-636b-4bf0-be65-8a378e9f80f4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\t\t\"The dog.\",\n",
        "\t\t\"The fox\"]\n",
        "\n",
        "# create the transform\n",
        "tfv = TfidfVectorizer()\n",
        "vector = tfv.fit_transform(text)\n",
        "\n",
        "# summarize\n",
        "print(\"-- Vocab\")\n",
        "print(tfv.vocabulary_)\n",
        "print(\"\\n-- IDF\")\n",
        "print(tfv.idf_)\n",
        "\n",
        "# encode document\n",
        "vector0 = tfv.transform([text[0]])\n",
        "\n",
        "# summarize encoded vector\n",
        "print(\"\\n-- Vector 0\")\n",
        "print(vector0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Vocab\n",
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "\n",
            "-- IDF\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n",
            "\n",
            "-- Vector 0\n",
            "  (0, 7)\t0.4298344050159891\n",
            "  (0, 6)\t0.3638864554802418\n",
            "  (0, 5)\t0.3638864554802418\n",
            "  (0, 4)\t0.3638864554802418\n",
            "  (0, 3)\t0.3638864554802418\n",
            "  (0, 2)\t0.27674502873103346\n",
            "  (0, 1)\t0.27674502873103346\n",
            "  (0, 0)\t0.3638864554802418\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}